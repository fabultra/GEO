#!/usr/bin/env python3
"""
Validation du pipeline de dÃ©couverte de compÃ©titeurs V3
Focus sur la validation de l'implÃ©mentation existante
"""
import sys
import os
import json
import logging
from datetime import datetime

# Ajouter le backend au path
sys.path.append('/app/backend')

# Configuration des logs
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_implementation_exists():
    """
    Test 1: VÃ©rifier que les nouveaux modules existent et sont importables
    """
    logger.info("ğŸ” Test 1: VÃ©rification de l'existence des modules")
    
    tests = []
    
    # Test CompetitorExtractor
    try:
        from utils.competitor_extractor import CompetitorExtractor
        logger.info("  âœ… CompetitorExtractor importÃ© avec succÃ¨s")
        
        # VÃ©rifier les mÃ©thodes clÃ©s
        methods = ['extract_from_visibility_results', 'filter_self_domain', '_normalize_url', '_extract_domain']
        for method in methods:
            if hasattr(CompetitorExtractor, method):
                logger.info(f"    âœ… MÃ©thode {method} prÃ©sente")
            else:
                logger.error(f"    âŒ MÃ©thode {method} manquante")
                tests.append(False)
        
        tests.append(True)
        
    except ImportError as e:
        logger.error(f"  âŒ Impossible d'importer CompetitorExtractor: {e}")
        tests.append(False)
    
    # Test CompetitorDiscovery
    try:
        from services.competitor_discovery import CompetitorDiscovery
        logger.info("  âœ… CompetitorDiscovery importÃ© avec succÃ¨s")
        
        # VÃ©rifier les mÃ©thodes clÃ©s
        discovery = CompetitorDiscovery()
        methods = ['discover_real_competitors', '_search_web_for_competitors', '_validate_and_score_competitors']
        for method in methods:
            if hasattr(discovery, method):
                logger.info(f"    âœ… MÃ©thode {method} prÃ©sente")
            else:
                logger.error(f"    âŒ MÃ©thode {method} manquante")
                tests.append(False)
        
        tests.append(True)
        
    except ImportError as e:
        logger.error(f"  âŒ Impossible d'importer CompetitorDiscovery: {e}")
        tests.append(False)
    
    return all(tests)

def test_server_integration():
    """
    Test 2: VÃ©rifier l'intÃ©gration dans server.py
    """
    logger.info("ğŸ” Test 2: VÃ©rification de l'intÃ©gration dans server.py")
    
    server_file = '/app/backend/server.py'
    
    if not os.path.exists(server_file):
        logger.error("  âŒ server.py non trouvÃ©")
        return False
    
    with open(server_file, 'r') as f:
        content = f.read()
    
    # VÃ©rifier les imports
    imports_to_check = [
        'from utils.competitor_extractor import CompetitorExtractor',
        'from services.competitor_discovery import competitor_discovery'
    ]
    
    integration_checks = []
    
    for import_line in imports_to_check:
        if import_line in content:
            logger.info(f"  âœ… Import trouvÃ©: {import_line}")
            integration_checks.append(True)
        else:
            logger.warning(f"  âš ï¸ Import non trouvÃ©: {import_line}")
            integration_checks.append(False)
    
    # VÃ©rifier l'utilisation dans le pipeline
    pipeline_keywords = [
        'CompetitorExtractor.extract_from_visibility_results',
        'competitor_discovery.discover_real_competitors',
        'Stage 1',
        'Stage 2',
        'Stage 3'
    ]
    
    for keyword in pipeline_keywords:
        if keyword in content:
            logger.info(f"  âœ… Pipeline keyword trouvÃ©: {keyword}")
            integration_checks.append(True)
        else:
            logger.warning(f"  âš ï¸ Pipeline keyword non trouvÃ©: {keyword}")
            integration_checks.append(False)
    
    # VÃ©rifier les lignes 1015-1050 mentionnÃ©es dans la review
    lines = content.split('\n')
    if len(lines) >= 1050:
        relevant_section = '\n'.join(lines[1014:1050])  # lignes 1015-1050 (0-indexed)
        
        if 'competitor' in relevant_section.lower():
            logger.info("  âœ… Section lignes 1015-1050 contient du code de compÃ©titeurs")
            integration_checks.append(True)
        else:
            logger.warning("  âš ï¸ Section lignes 1015-1050 ne semble pas contenir de code de compÃ©titeurs")
            integration_checks.append(False)
    else:
        logger.warning("  âš ï¸ server.py trop court pour vÃ©rifier les lignes 1015-1050")
        integration_checks.append(False)
    
    success_rate = sum(integration_checks) / len(integration_checks)
    logger.info(f"  ğŸ“Š Taux d'intÃ©gration: {success_rate:.1%}")
    
    return success_rate >= 0.7  # 70% minimum

def test_unit_tests_results():
    """
    Test 3: VÃ©rifier les rÃ©sultats des tests unitaires mentionnÃ©s (14/14)
    """
    logger.info("ğŸ” Test 3: VÃ©rification des tests unitaires")
    
    test_file = '/app/tests/test_competitor_discovery.py'
    
    if os.path.exists(test_file):
        logger.info("  âœ… Fichier de tests unitaires trouvÃ©")
        
        # Lire le contenu pour voir les tests
        with open(test_file, 'r') as f:
            content = f.read()
        
        # Compter les fonctions de test
        test_functions = [line for line in content.split('\n') if line.strip().startswith('def test_')]
        logger.info(f"  ğŸ“Š {len(test_functions)} fonctions de test trouvÃ©es")
        
        for i, test_func in enumerate(test_functions, 1):
            func_name = test_func.strip().split('(')[0].replace('def ', '')
            logger.info(f"    {i}. {func_name}")
        
        return len(test_functions) >= 10  # Au moins 10 tests
    else:
        logger.warning("  âš ï¸ Fichier de tests unitaires non trouvÃ©")
        return False

def test_new_fields_structure():
    """
    Test 4: VÃ©rifier la structure des nouveaux champs (score, type, reason, source)
    """
    logger.info("ğŸ” Test 4: Validation de la structure des nouveaux champs")
    
    try:
        from services.competitor_discovery import CompetitorDiscovery
        
        # CrÃ©er une instance pour examiner la structure
        discovery = CompetitorDiscovery()
        
        # VÃ©rifier les constantes/configurations
        config_checks = []
        
        if hasattr(discovery, 'threshold_direct'):
            logger.info(f"  âœ… threshold_direct configurÃ©: {discovery.threshold_direct}")
            config_checks.append(True)
        else:
            logger.warning("  âš ï¸ threshold_direct non configurÃ©")
            config_checks.append(False)
        
        if hasattr(discovery, 'threshold_indirect'):
            logger.info(f"  âœ… threshold_indirect configurÃ©: {discovery.threshold_indirect}")
            config_checks.append(True)
        else:
            logger.warning("  âš ï¸ threshold_indirect non configurÃ©")
            config_checks.append(False)
        
        # VÃ©rifier la mÃ©thode _calculate_relevance_score
        if hasattr(discovery, '_calculate_relevance_score'):
            logger.info("  âœ… MÃ©thode de calcul de score prÃ©sente")
            config_checks.append(True)
        else:
            logger.error("  âŒ MÃ©thode de calcul de score manquante")
            config_checks.append(False)
        
        # VÃ©rifier la mÃ©thode _generate_reason
        if hasattr(discovery, '_generate_reason'):
            logger.info("  âœ… MÃ©thode de gÃ©nÃ©ration de raison prÃ©sente")
            config_checks.append(True)
        else:
            logger.error("  âŒ MÃ©thode de gÃ©nÃ©ration de raison manquante")
            config_checks.append(False)
        
        return all(config_checks)
        
    except Exception as e:
        logger.error(f"  âŒ Erreur lors de la validation: {e}")
        return False

def test_backend_logs_for_pipeline():
    """
    Test 5: VÃ©rifier les logs backend pour des traces du pipeline 3 Ã©tages
    """
    logger.info("ğŸ” Test 5: Recherche de traces du pipeline dans les logs")
    
    log_files = [
        '/var/log/supervisor/backend.out.log',
        '/var/log/supervisor/backend.err.log'
    ]
    
    pipeline_keywords = [
        'Stage 1',
        'Stage 2', 
        'Stage 3',
        'competitor discovery',
        'CompetitorExtractor',
        'CompetitorDiscovery',
        'discover_real_competitors'
    ]
    
    found_keywords = set()
    
    for log_file in log_files:
        if os.path.exists(log_file):
            try:
                with open(log_file, 'r') as f:
                    content = f.read().lower()
                
                for keyword in pipeline_keywords:
                    if keyword.lower() in content:
                        found_keywords.add(keyword)
                        logger.info(f"  âœ… TrouvÃ© dans logs: {keyword}")
            
            except Exception as e:
                logger.warning(f"  âš ï¸ Impossible de lire {log_file}: {e}")
    
    logger.info(f"  ğŸ“Š Mots-clÃ©s trouvÃ©s: {len(found_keywords)}/{len(pipeline_keywords)}")
    
    return len(found_keywords) >= 3  # Au moins 3 mots-clÃ©s trouvÃ©s

def main():
    """
    Point d'entrÃ©e principal
    """
    logger.info("ğŸš€ Validation Pipeline DÃ©couverte CompÃ©titeurs V3")
    logger.info("="*60)
    
    results = {
        'test_start': datetime.now().isoformat(),
        'tests_run': 0,
        'tests_passed': 0,
        'tests_failed': 0,
        'details': {}
    }
    
    tests = [
        ('implementation_exists', test_implementation_exists),
        ('server_integration', test_server_integration),
        ('unit_tests_results', test_unit_tests_results),
        ('new_fields_structure', test_new_fields_structure),
        ('backend_logs_pipeline', test_backend_logs_for_pipeline)
    ]
    
    for test_name, test_func in tests:
        results['tests_run'] += 1
        logger.info(f"\n{'='*60}")
        
        try:
            if test_func():
                results['tests_passed'] += 1
                results['details'][test_name] = 'PASSED'
                logger.info(f"âœ… {test_name}: PASSED")
            else:
                results['tests_failed'] += 1
                results['details'][test_name] = 'FAILED'
                logger.info(f"âŒ {test_name}: FAILED")
        
        except Exception as e:
            results['tests_failed'] += 1
            results['details'][test_name] = f'ERROR: {str(e)}'
            logger.error(f"âŒ {test_name}: ERROR - {e}")
    
    # Rapport final
    results['test_end'] = datetime.now().isoformat()
    
    logger.info(f"\n{'='*60}")
    logger.info("ğŸ“‹ RAPPORT FINAL - VALIDATION PIPELINE")
    logger.info("="*60)
    logger.info(f"ğŸ“Š Tests exÃ©cutÃ©s: {results['tests_run']}")
    logger.info(f"âœ… Tests rÃ©ussis: {results['tests_passed']}")
    logger.info(f"âŒ Tests Ã©chouÃ©s: {results['tests_failed']}")
    
    success_rate = (results['tests_passed'] / results['tests_run']) * 100 if results['tests_run'] > 0 else 0
    logger.info(f"ğŸ“ˆ Taux de rÃ©ussite: {success_rate:.1f}%")
    
    logger.info("\nğŸ“‹ DÃ©tails par test:")
    for test_name, status in results['details'].items():
        status_icon = "âœ…" if status == "PASSED" else "âŒ"
        logger.info(f"  {status_icon} {test_name}: {status}")
    
    # Sauvegarder les rÃ©sultats
    with open('/app/competitor_pipeline_validation.json', 'w') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    logger.info(f"\nğŸ’¾ RÃ©sultats sauvegardÃ©s: /app/competitor_pipeline_validation.json")
    logger.info("="*60)
    
    return results['tests_failed'] == 0

if __name__ == "__main__":
    success = main()
    if success:
        print("\nğŸ‰ VALIDATION PIPELINE RÃ‰USSIE!")
        sys.exit(0)
    else:
        print("\nâš ï¸ VALIDATION PIPELINE PARTIELLE!")
        sys.exit(1)