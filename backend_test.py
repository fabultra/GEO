#!/usr/bin/env python3
"""
Test end-to-end du syst√®me de d√©couverte de comp√©titeurs V3
Validation compl√®te du pipeline 3 √©tages
"""
import asyncio
import json
import logging
import os
import sys
import time
from datetime import datetime
from typing import Dict, Any, List

import requests
from dotenv import load_dotenv

# Configuration
load_dotenv('/app/backend/.env')
BACKEND_URL = os.environ.get('REACT_APP_BACKEND_URL', 'https://geo-competitor-fix.preview.emergentagent.com')
API_BASE = f"{BACKEND_URL}/api"

# Configuration des logs
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CompetitorDiscoveryTester:
    """
    Testeur end-to-end pour le syst√®me de d√©couverte de comp√©titeurs
    """
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'Content-Type': 'application/json',
            'User-Agent': 'CompetitorDiscoveryTester/1.0'
        })
        
        # Sites de test recommand√©s
        self.test_sites = [
            {
                'url': 'sekoia.ca',
                'name': 'SEKOIA',
                'expected_industry': 'digital marketing',
                'expected_competitors_min': 2
            },
            {
                'url': 'maibec.com', 
                'name': 'Maibec',
                'expected_industry': 'manufacturing',
                'expected_competitors_min': 2
            }
        ]
        
        self.results = {
            'test_start': datetime.now().isoformat(),
            'tests_run': 0,
            'tests_passed': 0,
            'tests_failed': 0,
            'detailed_results': [],
            'critical_issues': [],
            'competitor_validation': []
        }
    
    def run_all_tests(self):
        """
        Lance tous les tests de validation
        """
        logger.info("üöÄ D√©marrage des tests end-to-end du syst√®me de d√©couverte de comp√©titeurs")
        logger.info(f"Backend URL: {API_BASE}")
        
        try:
            # Test 1: V√©rifier que l'API backend est accessible
            self.test_backend_health()
            
            # Test 2: Lancer une analyse compl√®te avec un site test
            test_site = self.test_sites[0]  # sekoia.ca
            analysis_result = self.test_complete_analysis(test_site)
            
            if analysis_result:
                # Test 3: Valider le pipeline de d√©couverte de comp√©titeurs
                self.test_competitor_discovery_pipeline(analysis_result)
                
                # Test 4: Valider les nouveaux champs des comp√©titeurs
                self.test_competitor_fields_validation(analysis_result)
                
                # Test 5: Valider que les URLs sont r√©elles et accessibles
                self.test_competitor_urls_accessibility(analysis_result)
                
                # Test 6: V√©rifier la sauvegarde MongoDB
                self.test_mongodb_storage(analysis_result)
            
            # G√©n√©rer le rapport final
            self.generate_final_report()
            
        except Exception as e:
            logger.error(f"‚ùå Erreur critique dans les tests: {e}")
            self.results['critical_issues'].append(f"Test suite failure: {e}")
            return False
        
        return self.results['tests_failed'] == 0
    
    def test_backend_health(self):
        """
        Test 1: V√©rifier que l'API backend est accessible
        """
        logger.info("üîç Test 1: V√©rification de l'accessibilit√© du backend")
        self.results['tests_run'] += 1
        
        try:
            response = self.session.get(f"{API_BASE}/")
            
            if response.status_code == 200:
                logger.info("‚úÖ Backend accessible")
                self.results['tests_passed'] += 1
                return True
            else:
                logger.error(f"‚ùå Backend non accessible: {response.status_code}")
                self.results['tests_failed'] += 1
                self.results['critical_issues'].append(f"Backend not accessible: HTTP {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Erreur de connexion au backend: {e}")
            self.results['tests_failed'] += 1
            self.results['critical_issues'].append(f"Backend connection error: {e}")
            return False
    
    def test_complete_analysis(self, test_site: Dict[str, Any]) -> Dict[str, Any]:
        """
        Test 2: Lancer une analyse compl√®te avec un site test
        """
        logger.info(f"üîç Test 2: Analyse compl√®te de {test_site['url']}")
        self.results['tests_run'] += 1
        
        try:
            # Cr√©er un lead de test
            lead_data = {
                "firstName": "Test",
                "lastName": "CompetitorDiscovery",
                "email": "test@example.com",
                "company": test_site['name'],
                "url": test_site['url'],
                "consent": True
            }
            
            logger.info(f"üìù Cr√©ation du lead pour {test_site['url']} (analyse automatique)")
            lead_response = self.session.post(f"{API_BASE}/leads", json=lead_data)
            
            if lead_response.status_code != 200:
                logger.error(f"‚ùå √âchec cr√©ation lead: {lead_response.status_code}")
                logger.error(f"Response: {lead_response.text}")
                self.results['tests_failed'] += 1
                return None
            
            lead = lead_response.json()
            lead_id = lead['id']
            logger.info(f"‚úÖ Lead cr√©√©: {lead_id}")
            
            # L'analyse est lanc√©e automatiquement, r√©cup√©rer le job
            logger.info("üîç Recherche du job d'analyse...")
            
            # Attendre un peu que le job soit cr√©√©
            time.sleep(2)
            
            # R√©cup√©rer tous les leads pour trouver le job
            leads_response = self.session.get(f"{API_BASE}/leads")
            if leads_response.status_code == 200:
                leads = leads_response.json()
                current_lead = None
                for lead_data in leads:
                    if lead_data['id'] == lead_id:
                        current_lead = lead_data
                        break
                
                if current_lead and current_lead.get('latestJob'):
                    job = current_lead['latestJob']
                    job_id = job['id']
                    logger.info(f"‚úÖ Job d'analyse trouv√©: {job_id}")
                else:
                    logger.error("‚ùå Aucun job d'analyse trouv√©")
                    self.results['tests_failed'] += 1
                    return None
            else:
                logger.error(f"‚ùå Impossible de r√©cup√©rer les leads: {leads_response.status_code}")
                self.results['tests_failed'] += 1
                return None
            
            # Attendre la completion (max 10 minutes)
            max_wait = 600  # 10 minutes
            wait_time = 0
            poll_interval = 15  # 15 secondes
            
            logger.info("‚è≥ Attente de la completion de l'analyse...")
            
            while wait_time < max_wait:
                time.sleep(poll_interval)
                wait_time += poll_interval
                
                # V√©rifier le statut
                status_response = self.session.get(f"{API_BASE}/jobs/{job_id}")
                if status_response.status_code == 200:
                    job_status = status_response.json()
                    status = job_status.get('status', 'unknown')
                    progress = job_status.get('progress', 0)
                    
                    logger.info(f"üìä Statut: {status} ({progress}%)")
                    
                    if status == 'completed':
                        report_id = job_status.get('reportId')
                        if report_id:
                            logger.info(f"‚úÖ Analyse termin√©e! Report ID: {report_id}")
                            
                            # R√©cup√©rer le rapport
                            report_response = self.session.get(f"{API_BASE}/reports/{report_id}")
                            if report_response.status_code == 200:
                                report = report_response.json()
                                self.results['tests_passed'] += 1
                                
                                # Sauvegarder pour debug
                                with open(f'/app/test_report_{report_id}.json', 'w') as f:
                                    json.dump(report, f, indent=2, ensure_ascii=False)
                                
                                return {
                                    'job_id': job_id,
                                    'report_id': report_id,
                                    'report': report,
                                    'test_site': test_site
                                }
                            else:
                                logger.error(f"‚ùå Impossible de r√©cup√©rer le rapport: {report_response.status_code}")
                        break
                    elif status == 'failed':
                        error = job_status.get('error', 'Unknown error')
                        logger.error(f"‚ùå Analyse √©chou√©e: {error}")
                        self.results['critical_issues'].append(f"Analysis failed for {test_site['url']}: {error}")
                        break
                else:
                    logger.warning(f"‚ö†Ô∏è Impossible de v√©rifier le statut: {status_response.status_code}")
            
            if wait_time >= max_wait:
                logger.error("‚ùå Timeout: analyse non termin√©e dans les temps")
                self.results['critical_issues'].append(f"Analysis timeout for {test_site['url']}")
            
            self.results['tests_failed'] += 1
            return None
            
        except Exception as e:
            logger.error(f"‚ùå Erreur dans l'analyse compl√®te: {e}")
            self.results['tests_failed'] += 1
            self.results['critical_issues'].append(f"Complete analysis error: {e}")
            return None
    
    def test_competitor_discovery_pipeline(self, analysis_result: Dict[str, Any]):
        """
        Test 3: Valider le pipeline de d√©couverte de comp√©titeurs
        """
        logger.info("üîç Test 3: Validation du pipeline de d√©couverte de comp√©titeurs")
        self.results['tests_run'] += 1
        
        try:
            report = analysis_result['report']
            
            # V√©rifier la pr√©sence des donn√©es de competitive intelligence
            competitive_intel = None
            
            # Chercher dans visibility_results
            visibility_results = report.get('visibility_results')
            if visibility_results and isinstance(visibility_results, dict):
                competitive_intel = visibility_results.get('competitive_intelligence')
            
            # Chercher dans analysis (fallback)
            if not competitive_intel:
                analysis = report.get('analysis')
                if analysis and isinstance(analysis, dict):
                    competitive_intel = analysis.get('competitive_intelligence')
            
            if not competitive_intel:
                logger.error("‚ùå Aucune donn√©e de competitive intelligence trouv√©e")
                self.results['tests_failed'] += 1
                self.results['critical_issues'].append("No competitive intelligence data found")
                return False
            
            logger.info(f"‚úÖ Donn√©es de competitive intelligence trouv√©es: {type(competitive_intel)}")
            
            # V√©rifier les m√©triques de base
            competitors_analyzed = competitive_intel.get('competitors_analyzed', 0)
            logger.info(f"üìä Comp√©titeurs analys√©s: {competitors_analyzed}")
            
            if competitors_analyzed > 0:
                logger.info("‚úÖ Pipeline de d√©couverte fonctionnel")
                self.results['tests_passed'] += 1
                
                # V√©rifier la structure des analyses
                analyses = competitive_intel.get('analyses', [])
                if analyses:
                    logger.info(f"üìã {len(analyses)} analyses de comp√©titeurs disponibles")
                    
                    # Examiner la premi√®re analyse
                    first_analysis = analyses[0]
                    logger.info(f"üîç Premier comp√©titeur: {first_analysis.get('url', 'N/A')}")
                    
                return True
            else:
                logger.warning("‚ö†Ô∏è Aucun comp√©titeur analys√© - peut √™tre normal selon les donn√©es de visibilit√©")
                self.results['tests_passed'] += 1  # Pas un √©chec critique
                return True
                
        except Exception as e:
            logger.error(f"‚ùå Erreur dans la validation du pipeline: {e}")
            self.results['tests_failed'] += 1
            self.results['critical_issues'].append(f"Pipeline validation error: {e}")
            return False
    
    def test_competitor_fields_validation(self, analysis_result: Dict[str, Any]):
        """
        Test 4: Valider les nouveaux champs des comp√©titeurs (score, type, reason, source)
        """
        logger.info("üîç Test 4: Validation des nouveaux champs des comp√©titeurs")
        self.results['tests_run'] += 1
        
        try:
            report = analysis_result['report']
            
            # Chercher les donn√©es de comp√©titeurs avec les nouveaux champs
            competitors_with_new_fields = []
            
            # V√©rifier dans visibility_results
            visibility_results = report.get('visibility_results')
            if visibility_results and isinstance(visibility_results, dict):
                competitive_intel = visibility_results.get('competitive_intelligence', {})
                analyses = competitive_intel.get('analyses', [])
                
                for analysis in analyses:
                    # V√©rifier si les nouveaux champs sont pr√©sents
                    has_score = 'score' in analysis
                    has_type = 'type' in analysis
                    has_reason = 'reason' in analysis
                    has_source = 'source' in analysis
                    
                    if has_score or has_type or has_reason or has_source:
                        competitors_with_new_fields.append({
                            'url': analysis.get('url', 'N/A'),
                            'score': analysis.get('score'),
                            'type': analysis.get('type'),
                            'reason': analysis.get('reason'),
                            'source': analysis.get('source'),
                            'has_all_fields': has_score and has_type and has_reason and has_source
                        })
            
            if competitors_with_new_fields:
                logger.info(f"‚úÖ {len(competitors_with_new_fields)} comp√©titeurs avec nouveaux champs trouv√©s")
                
                # Analyser les champs
                for i, comp in enumerate(competitors_with_new_fields, 1):
                    logger.info(f"  {i}. {comp['url']}")
                    logger.info(f"     Score: {comp['score']}")
                    logger.info(f"     Type: {comp['type']}")
                    logger.info(f"     Reason: {comp['reason']}")
                    logger.info(f"     Source: {comp['source']}")
                    logger.info(f"     Tous champs: {'‚úÖ' if comp['has_all_fields'] else '‚ùå'}")
                
                # V√©rifier si au moins un comp√©titeur a tous les champs
                complete_competitors = [c for c in competitors_with_new_fields if c['has_all_fields']]
                
                if complete_competitors:
                    logger.info(f"‚úÖ {len(complete_competitors)} comp√©titeurs avec tous les nouveaux champs")
                    self.results['tests_passed'] += 1
                    return True
                else:
                    logger.warning("‚ö†Ô∏è Aucun comp√©titeur avec tous les nouveaux champs")
                    self.results['tests_failed'] += 1
                    self.results['critical_issues'].append("No competitors with all new fields (score, type, reason, source)")
                    return False
            else:
                logger.warning("‚ö†Ô∏è Aucun comp√©titeur avec nouveaux champs trouv√©")
                self.results['tests_failed'] += 1
                self.results['critical_issues'].append("No competitors with new fields found")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Erreur dans la validation des champs: {e}")
            self.results['tests_failed'] += 1
            self.results['critical_issues'].append(f"Fields validation error: {e}")
            return False
    
    def test_competitor_urls_accessibility(self, analysis_result: Dict[str, Any]):
        """
        Test 5: Valider que les URLs des comp√©titeurs sont r√©elles et accessibles
        """
        logger.info("üîç Test 5: Validation de l'accessibilit√© des URLs des comp√©titeurs")
        self.results['tests_run'] += 1
        
        try:
            report = analysis_result['report']
            competitor_urls = []
            
            # Extraire toutes les URLs de comp√©titeurs
            visibility_results = report.get('visibility_results')
            if visibility_results and isinstance(visibility_results, dict):
                competitive_intel = visibility_results.get('competitive_intelligence', {})
                analyses = competitive_intel.get('analyses', [])
                
                for analysis in analyses:
                    url = analysis.get('url')
                    if url:
                        competitor_urls.append(url)
            
            if not competitor_urls:
                logger.warning("‚ö†Ô∏è Aucune URL de comp√©titeur √† tester")
                self.results['tests_passed'] += 1  # Pas un √©chec si pas de comp√©titeurs
                return True
            
            logger.info(f"üåê Test d'accessibilit√© de {len(competitor_urls)} URLs")
            
            accessible_count = 0
            inaccessible_urls = []
            
            for url in competitor_urls:
                try:
                    # Test HEAD request rapide
                    response = requests.head(
                        url, 
                        timeout=10, 
                        allow_redirects=True,
                        headers={'User-Agent': 'Mozilla/5.0 (compatible; CompetitorTester/1.0)'}
                    )
                    
                    if response.status_code < 400:
                        logger.info(f"  ‚úÖ {url} - Accessible ({response.status_code})")
                        accessible_count += 1
                        
                        # Enregistrer pour le rapport
                        self.results['competitor_validation'].append({
                            'url': url,
                            'accessible': True,
                            'status_code': response.status_code,
                            'is_real': True
                        })
                    else:
                        logger.warning(f"  ‚ö†Ô∏è {url} - Status {response.status_code}")
                        inaccessible_urls.append(url)
                        
                        self.results['competitor_validation'].append({
                            'url': url,
                            'accessible': False,
                            'status_code': response.status_code,
                            'is_real': False
                        })
                        
                except Exception as e:
                    logger.error(f"  ‚ùå {url} - Erreur: {e}")
                    inaccessible_urls.append(url)
                    
                    self.results['competitor_validation'].append({
                        'url': url,
                        'accessible': False,
                        'error': str(e),
                        'is_real': False
                    })
            
            # √âvaluer les r√©sultats
            accessibility_rate = accessible_count / len(competitor_urls) if competitor_urls else 0
            logger.info(f"üìä Taux d'accessibilit√©: {accessibility_rate:.1%} ({accessible_count}/{len(competitor_urls)})")
            
            if accessibility_rate >= 0.8:  # 80% minimum
                logger.info("‚úÖ Taux d'accessibilit√© satisfaisant")
                self.results['tests_passed'] += 1
                return True
            else:
                logger.error(f"‚ùå Taux d'accessibilit√© insuffisant: {accessibility_rate:.1%}")
                self.results['tests_failed'] += 1
                self.results['critical_issues'].append(f"Low URL accessibility rate: {accessibility_rate:.1%}")
                
                if inaccessible_urls:
                    logger.error(f"URLs inaccessibles: {inaccessible_urls}")
                
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Erreur dans le test d'accessibilit√©: {e}")
            self.results['tests_failed'] += 1
            self.results['critical_issues'].append(f"URL accessibility test error: {e}")
            return False
    
    def test_mongodb_storage(self, analysis_result: Dict[str, Any]):
        """
        Test 6: V√©rifier la sauvegarde MongoDB avec les nouveaux champs
        """
        logger.info("üîç Test 6: Validation de la sauvegarde MongoDB")
        self.results['tests_run'] += 1
        
        try:
            report_id = analysis_result['report_id']
            
            # R√©cup√©rer le rapport depuis l'API (qui lit MongoDB)
            report_response = self.session.get(f"{API_BASE}/reports/{report_id}")
            
            if report_response.status_code == 200:
                report = report_response.json()
                
                # V√©rifier que les donn√©es de competitive intelligence sont sauvegard√©es
                has_competitive_data = False
                
                if 'visibility_results' in report:
                    visibility_results = report['visibility_results']
                    if isinstance(visibility_results, dict) and 'competitive_intelligence' in visibility_results:
                        competitive_intel = visibility_results['competitive_intelligence']
                        if competitive_intel and competitive_intel.get('competitors_analyzed', 0) > 0:
                            has_competitive_data = True
                
                if has_competitive_data:
                    logger.info("‚úÖ Donn√©es de competitive intelligence sauvegard√©es en MongoDB")
                    self.results['tests_passed'] += 1
                    return True
                else:
                    logger.warning("‚ö†Ô∏è Pas de donn√©es de competitive intelligence en MongoDB")
                    self.results['tests_passed'] += 1  # Pas critique si pas de comp√©titeurs
                    return True
            else:
                logger.error(f"‚ùå Impossible de r√©cup√©rer le rapport depuis MongoDB: {report_response.status_code}")
                self.results['tests_failed'] += 1
                self.results['critical_issues'].append(f"MongoDB retrieval failed: HTTP {report_response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Erreur dans le test MongoDB: {e}")
            self.results['tests_failed'] += 1
            self.results['critical_issues'].append(f"MongoDB test error: {e}")
            return False
    
    def generate_final_report(self):
        """
        G√©n√®re le rapport final des tests
        """
        self.results['test_end'] = datetime.now().isoformat()
        
        logger.info("\n" + "="*80)
        logger.info("üìã RAPPORT FINAL - TESTS SYST√àME DE D√âCOUVERTE DE COMP√âTITEURS")
        logger.info("="*80)
        
        logger.info(f"üïê Dur√©e: {self.results['test_start']} ‚Üí {self.results['test_end']}")
        logger.info(f"üìä Tests ex√©cut√©s: {self.results['tests_run']}")
        logger.info(f"‚úÖ Tests r√©ussis: {self.results['tests_passed']}")
        logger.info(f"‚ùå Tests √©chou√©s: {self.results['tests_failed']}")
        
        success_rate = (self.results['tests_passed'] / self.results['tests_run']) * 100 if self.results['tests_run'] > 0 else 0
        logger.info(f"üìà Taux de r√©ussite: {success_rate:.1f}%")
        
        if self.results['critical_issues']:
            logger.info("\nüö® PROBL√àMES CRITIQUES:")
            for issue in self.results['critical_issues']:
                logger.info(f"  ‚Ä¢ {issue}")
        
        if self.results['competitor_validation']:
            logger.info(f"\nüåê VALIDATION DES COMP√âTITEURS ({len(self.results['competitor_validation'])}):")
            for comp in self.results['competitor_validation']:
                status = "‚úÖ" if comp['accessible'] else "‚ùå"
                logger.info(f"  {status} {comp['url']} - {comp.get('status_code', 'Error')}")
        
        # Sauvegarder le rapport complet
        with open('/app/competitor_discovery_test_results.json', 'w') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"\nüíæ Rapport d√©taill√© sauvegard√©: /app/competitor_discovery_test_results.json")
        logger.info("="*80)


def main():
    """
    Point d'entr√©e principal
    """
    print("üöÄ Test End-to-End - Syst√®me de D√©couverte de Comp√©titeurs V3")
    print("="*70)
    
    tester = CompetitorDiscoveryTester()
    success = tester.run_all_tests()
    
    if success:
        print("\nüéâ TOUS LES TESTS SONT PASS√âS!")
        return 0
    else:
        print("\n‚ùå CERTAINS TESTS ONT √âCHOU√â!")
        return 1


if __name__ == "__main__":
    sys.exit(main())