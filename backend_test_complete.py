#!/usr/bin/env python3
"""
Test complet du backend de la plateforme GEO SaaS
Validation de tous les modules selon la review request
"""
import asyncio
import json
import logging
import os
import sys
import time
from datetime import datetime
from typing import Dict, Any, List, Optional

import requests
from dotenv import load_dotenv

# Configuration
load_dotenv('/app/backend/.env')
BACKEND_URL = os.environ.get('REACT_APP_BACKEND_URL', 'https://issue-resolver-41.preview.emergentagent.com')
API_BASE = f"{BACKEND_URL}/api"

# Configuration des logs
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class GEOSaaSBackendTester:
    """
    Testeur complet pour le backend de la plateforme GEO SaaS
    """
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'Content-Type': 'application/json',
            'User-Agent': 'GEOSaaSBackendTester/1.0'
        })
        
        # URL de test selon la review request
        self.test_url = 'sekoia.ca'
        
        self.results = {
            'test_start': datetime.now().isoformat(),
            'backend_url': API_BASE,
            'test_url': self.test_url,
            'tests_run': 0,
            'tests_passed': 0,
            'tests_failed': 0,
            'critical_issues': [],
            'minor_issues': [],
            'modules_validation': {},
            'downloads_validation': {},
            'performance_metrics': {}
        }
        
        # Variables pour stocker les IDs pendant les tests
        self.job_id = None
        self.report_id = None
        self.lead_id = None
    
    def run_complete_tests(self):
        """
        Lance tous les tests selon la review request
        """
        logger.info("ğŸš€ DÃ‰MARRAGE DES TESTS COMPLETS DU BACKEND GEO SAAS")
        logger.info(f"Backend URL: {API_BASE}")
        logger.info(f"URL de test: {self.test_url}")
        logger.info("="*80)
        
        try:
            # Test 1: SantÃ© de l'API (GET /api/)
            if not self.test_api_health():
                return False
            
            # Test 2: CrÃ©ation d'un lead avec URL valide (POST /api/leads)
            if not self.test_lead_creation():
                return False
            
            # Test 3: VÃ©rifier le statut du job d'analyse (GET /api/jobs/{job_id})
            if not self.test_job_status():
                return False
            
            # Test 4: Attendre que l'analyse soit complÃ¨te (polling avec timeout 180s)
            if not self.test_analysis_completion():
                return False
            
            # Test 5: VÃ©rifier que le rapport est gÃ©nÃ©rÃ© (GET /api/reports/{report_id})
            if not self.test_report_generation():
                return False
            
            # Test 6: VÃ©rifier la prÃ©sence de tous les modules dans le rapport
            if not self.test_modules_validation():
                return False
            
            # Test 7: VÃ©rifier les tÃ©lÃ©chargements (Dashboard HTML, Word DOCX, PDF)
            if not self.test_downloads():
                return False
            
            # Test 8: VÃ©rifier les logs backend pour erreurs
            self.check_backend_logs()
            
            # GÃ©nÃ©rer le rapport final
            self.generate_final_report()
            
        except Exception as e:
            logger.error(f"âŒ Erreur critique dans les tests: {e}")
            self.results['critical_issues'].append(f"Test suite failure: {e}")
            return False
        
        return self.results['tests_failed'] == 0
    
    def test_api_health(self) -> bool:
        """
        Test 1: SantÃ© de l'API (GET /api/)
        """
        logger.info("ğŸ” Test 1: SantÃ© de l'API (GET /api/)")
        self.results['tests_run'] += 1
        
        try:
            start_time = time.time()
            response = self.session.get(f"{API_BASE}/")
            response_time = time.time() - start_time
            
            self.results['performance_metrics']['api_health_response_time'] = response_time
            
            if response.status_code == 200:
                data = response.json()
                logger.info(f"âœ… API accessible - {data.get('message', 'N/A')} v{data.get('version', 'N/A')}")
                logger.info(f"â±ï¸ Temps de rÃ©ponse: {response_time:.2f}s")
                self.results['tests_passed'] += 1
                return True
            else:
                logger.error(f"âŒ API non accessible: HTTP {response.status_code}")
                logger.error(f"Response: {response.text}")
                self.results['tests_failed'] += 1
                self.results['critical_issues'].append(f"API health check failed: HTTP {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Erreur de connexion Ã  l'API: {e}")
            self.results['tests_failed'] += 1
            self.results['critical_issues'].append(f"API connection error: {e}")
            return False
    
    def test_lead_creation(self) -> bool:
        """
        Test 2: CrÃ©ation d'un lead avec URL valide (POST /api/leads)
        """
        logger.info("ğŸ” Test 2: CrÃ©ation d'un lead avec URL valide (POST /api/leads)")
        self.results['tests_run'] += 1
        
        try:
            # DonnÃ©es rÃ©elles selon la review request
            lead_data = {
                "firstName": "Jean",
                "lastName": "Tremblay",
                "email": "jean.tremblay@sekoia.ca",
                "company": "SEKOIA",
                "url": self.test_url,
                "consent": True
            }
            
            logger.info(f"ğŸ“ CrÃ©ation du lead pour {self.test_url}")
            start_time = time.time()
            response = self.session.post(f"{API_BASE}/leads", json=lead_data)
            response_time = time.time() - start_time
            
            self.results['performance_metrics']['lead_creation_response_time'] = response_time
            
            if response.status_code == 200:
                lead = response.json()
                self.lead_id = lead['id']
                logger.info(f"âœ… Lead crÃ©Ã© avec succÃ¨s - ID: {self.lead_id}")
                logger.info(f"â±ï¸ Temps de rÃ©ponse: {response_time:.2f}s")
                
                # VÃ©rifier les champs obligatoires
                required_fields = ['id', 'firstName', 'lastName', 'email', 'url', 'createdAt']
                missing_fields = [field for field in required_fields if field not in lead]
                
                if missing_fields:
                    logger.warning(f"âš ï¸ Champs manquants dans la rÃ©ponse: {missing_fields}")
                    self.results['minor_issues'].append(f"Missing fields in lead response: {missing_fields}")
                
                self.results['tests_passed'] += 1
                return True
            else:
                logger.error(f"âŒ Ã‰chec crÃ©ation lead: HTTP {response.status_code}")
                logger.error(f"Response: {response.text}")
                self.results['tests_failed'] += 1
                self.results['critical_issues'].append(f"Lead creation failed: HTTP {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Erreur dans la crÃ©ation du lead: {e}")
            self.results['tests_failed'] += 1
            self.results['critical_issues'].append(f"Lead creation error: {e}")
            return False
    
    def test_job_status(self) -> bool:
        """
        Test 3: VÃ©rifier le statut du job d'analyse crÃ©Ã© (GET /api/jobs/{job_id})
        """
        logger.info("ğŸ” Test 3: VÃ©rification du statut du job d'analyse")
        self.results['tests_run'] += 1
        
        try:
            # Attendre un peu que le job soit crÃ©Ã©
            time.sleep(3)
            
            # RÃ©cupÃ©rer le job via l'endpoint leads
            logger.info("ğŸ” Recherche du job d'analyse...")
            leads_response = self.session.get(f"{API_BASE}/leads")
            
            if leads_response.status_code != 200:
                logger.error(f"âŒ Impossible de rÃ©cupÃ©rer les leads: HTTP {leads_response.status_code}")
                self.results['tests_failed'] += 1
                self.results['critical_issues'].append(f"Failed to retrieve leads: HTTP {leads_response.status_code}")
                return False
            
            leads = leads_response.json()
            current_lead = None
            
            for lead_data in leads:
                if lead_data['id'] == self.lead_id:
                    current_lead = lead_data
                    break
            
            if not current_lead:
                logger.error(f"âŒ Lead {self.lead_id} non trouvÃ©")
                self.results['tests_failed'] += 1
                self.results['critical_issues'].append(f"Lead {self.lead_id} not found")
                return False
            
            if not current_lead.get('latestJob'):
                logger.error("âŒ Aucun job d'analyse trouvÃ© pour ce lead")
                self.results['tests_failed'] += 1
                self.results['critical_issues'].append("No analysis job found for lead")
                return False
            
            job = current_lead['latestJob']
            self.job_id = job['id']
            
            logger.info(f"âœ… Job d'analyse trouvÃ© - ID: {self.job_id}")
            logger.info(f"ğŸ“Š Statut initial: {job.get('status', 'unknown')} ({job.get('progress', 0)}%)")
            
            # Test direct de l'endpoint job
            job_response = self.session.get(f"{API_BASE}/jobs/{self.job_id}")
            
            if job_response.status_code == 200:
                job_data = job_response.json()
                logger.info(f"âœ… Endpoint /jobs/{self.job_id} accessible")
                
                # VÃ©rifier les champs obligatoires
                required_fields = ['id', 'leadId', 'url', 'status', 'progress', 'createdAt', 'updatedAt']
                missing_fields = [field for field in required_fields if field not in job_data]
                
                if missing_fields:
                    logger.warning(f"âš ï¸ Champs manquants dans le job: {missing_fields}")
                    self.results['minor_issues'].append(f"Missing fields in job response: {missing_fields}")
                
                self.results['tests_passed'] += 1
                return True
            else:
                logger.error(f"âŒ Endpoint job inaccessible: HTTP {job_response.status_code}")
                self.results['tests_failed'] += 1
                self.results['critical_issues'].append(f"Job endpoint failed: HTTP {job_response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Erreur dans la vÃ©rification du job: {e}")
            self.results['tests_failed'] += 1
            self.results['critical_issues'].append(f"Job status check error: {e}")
            return False
    
    def test_analysis_completion(self) -> bool:
        """
        Test 4: Attendre que l'analyse soit complÃ¨te ou Ã©choue (polling avec timeout 180 secondes)
        """
        logger.info("ğŸ” Test 4: Attente de la completion de l'analyse (timeout 180s)")
        self.results['tests_run'] += 1
        
        try:
            max_wait = 180  # 180 secondes selon la review request
            wait_time = 0
            poll_interval = 10  # 10 secondes
            
            logger.info("â³ Polling du statut de l'analyse...")
            start_time = time.time()
            
            while wait_time < max_wait:
                time.sleep(poll_interval)
                wait_time += poll_interval
                
                # VÃ©rifier le statut
                status_response = self.session.get(f"{API_BASE}/jobs/{self.job_id}")
                
                if status_response.status_code != 200:
                    logger.warning(f"âš ï¸ Impossible de vÃ©rifier le statut: HTTP {status_response.status_code}")
                    continue
                
                job_status = status_response.json()
                status = job_status.get('status', 'unknown')
                progress = job_status.get('progress', 0)
                
                logger.info(f"ğŸ“Š Statut: {status} ({progress}%) - Temps Ã©coulÃ©: {wait_time}s")
                
                if status == 'completed':
                    completion_time = time.time() - start_time
                    self.results['performance_metrics']['analysis_completion_time'] = completion_time
                    
                    self.report_id = job_status.get('reportId')
                    if self.report_id:
                        logger.info(f"âœ… Analyse terminÃ©e en {completion_time:.1f}s - Report ID: {self.report_id}")
                        self.results['tests_passed'] += 1
                        return True
                    else:
                        logger.error("âŒ Analyse terminÃ©e mais aucun reportId")
                        self.results['tests_failed'] += 1
                        self.results['critical_issues'].append("Analysis completed but no reportId")
                        return False
                        
                elif status == 'failed':
                    error = job_status.get('error', 'Unknown error')
                    logger.error(f"âŒ Analyse Ã©chouÃ©e: {error}")
                    self.results['tests_failed'] += 1
                    self.results['critical_issues'].append(f"Analysis failed: {error}")
                    return False
                
                elif status == 'processing':
                    # Continuer le polling
                    continue
                else:
                    logger.warning(f"âš ï¸ Statut inattendu: {status}")
            
            # Timeout atteint
            logger.error(f"âŒ TIMEOUT: Analyse non terminÃ©e aprÃ¨s {max_wait}s")
            self.results['tests_failed'] += 1
            self.results['critical_issues'].append(f"Analysis timeout after {max_wait}s")
            return False
            
        except Exception as e:
            logger.error(f"âŒ Erreur dans l'attente de completion: {e}")
            self.results['tests_failed'] += 1
            self.results['critical_issues'].append(f"Analysis completion error: {e}")
            return False
    
    def test_report_generation(self) -> bool:
        """
        Test 5: VÃ©rifier que le rapport est gÃ©nÃ©rÃ© (GET /api/reports/{report_id})
        """
        logger.info("ğŸ” Test 5: VÃ©rification de la gÃ©nÃ©ration du rapport")
        self.results['tests_run'] += 1
        
        try:
            if not self.report_id:
                logger.error("âŒ Aucun report_id disponible")
                self.results['tests_failed'] += 1
                self.results['critical_issues'].append("No report_id available")
                return False
            
            logger.info(f"ğŸ“Š RÃ©cupÃ©ration du rapport {self.report_id}")
            start_time = time.time()
            report_response = self.session.get(f"{API_BASE}/reports/{self.report_id}")
            response_time = time.time() - start_time
            
            self.results['performance_metrics']['report_retrieval_time'] = response_time
            
            if report_response.status_code == 200:
                report = report_response.json()
                logger.info(f"âœ… Rapport rÃ©cupÃ©rÃ© avec succÃ¨s")
                logger.info(f"â±ï¸ Temps de rÃ©ponse: {response_time:.2f}s")
                
                # VÃ©rifier les champs obligatoires du rapport
                required_fields = ['id', 'leadId', 'url', 'scores', 'createdAt']
                missing_fields = [field for field in required_fields if field not in report]
                
                if missing_fields:
                    logger.warning(f"âš ï¸ Champs manquants dans le rapport: {missing_fields}")
                    self.results['minor_issues'].append(f"Missing fields in report: {missing_fields}")
                
                # VÃ©rifier la structure des scores
                scores = report.get('scores', {})
                if scores:
                    score_fields = ['structure', 'answerability', 'readability', 'eeat', 'educational', 'thematic', 'aiOptimization', 'visibility', 'global_score']
                    missing_score_fields = [field for field in score_fields if field not in scores]
                    
                    if missing_score_fields:
                        logger.warning(f"âš ï¸ Champs de score manquants: {missing_score_fields}")
                        self.results['minor_issues'].append(f"Missing score fields: {missing_score_fields}")
                    
                    logger.info(f"ğŸ“Š Score global: {scores.get('global_score', 'N/A')}/10")
                
                # Sauvegarder le rapport pour analyse dÃ©taillÃ©e
                with open(f'/app/test_report_{self.report_id}.json', 'w', encoding='utf-8') as f:
                    json.dump(report, f, indent=2, ensure_ascii=False)
                
                logger.info(f"ğŸ’¾ Rapport sauvegardÃ©: /app/test_report_{self.report_id}.json")
                
                self.results['tests_passed'] += 1
                return True
            else:
                logger.error(f"âŒ Impossible de rÃ©cupÃ©rer le rapport: HTTP {report_response.status_code}")
                logger.error(f"Response: {report_response.text}")
                self.results['tests_failed'] += 1
                self.results['critical_issues'].append(f"Report retrieval failed: HTTP {report_response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Erreur dans la rÃ©cupÃ©ration du rapport: {e}")
            self.results['tests_failed'] += 1
            self.results['critical_issues'].append(f"Report generation test error: {e}")
            return False
    
    def test_modules_validation(self) -> bool:
        """
        Test 6: VÃ©rifier la prÃ©sence de tous les modules dans le rapport
        """
        logger.info("ğŸ” Test 6: Validation de tous les modules dans le rapport")
        self.results['tests_run'] += 1
        
        try:
            # RÃ©cupÃ©rer le rapport
            report_response = self.session.get(f"{API_BASE}/reports/{self.report_id}")
            
            if report_response.status_code != 200:
                logger.error(f"âŒ Impossible de rÃ©cupÃ©rer le rapport pour validation: HTTP {report_response.status_code}")
                self.results['tests_failed'] += 1
                self.results['critical_issues'].append("Cannot retrieve report for module validation")
                return False
            
            report = report_response.json()
            
            # Modules requis selon la review request
            required_modules = {
                'visibility_results': 'Module 1: Tests de visibilitÃ©',
                'semantic_analysis': 'Module 5: Analyse sÃ©mantique',
                'competitive_intelligence': 'Module 3: Intelligence compÃ©titive',
                'schemas': 'Module 4: GÃ©nÃ©ration de schemas',
                'generated_articles': 'Module 2: Articles gÃ©nÃ©rÃ©s'
            }
            
            logger.info("ğŸ“‹ Validation des modules requis:")
            
            modules_found = 0
            modules_missing = []
            
            for module_key, module_name in required_modules.items():
                module_data = None
                
                # Chercher le module dans diffÃ©rents endroits du rapport
                if module_key in report:
                    module_data = report[module_key]
                elif 'visibility_results' in report and isinstance(report['visibility_results'], dict):
                    if module_key in report['visibility_results']:
                        module_data = report['visibility_results'][module_key]
                
                if module_data is not None and module_data != {}:
                    logger.info(f"  âœ… {module_name}: PrÃ©sent")
                    modules_found += 1
                    
                    # Validation spÃ©cifique par module
                    self.validate_specific_module(module_key, module_data, module_name)
                else:
                    logger.error(f"  âŒ {module_name}: MANQUANT")
                    modules_missing.append(module_name)
                
                self.results['modules_validation'][module_key] = {
                    'name': module_name,
                    'present': module_data is not None and module_data != {},
                    'data_size': len(str(module_data)) if module_data else 0
                }
            
            # Ã‰valuation globale
            total_modules = len(required_modules)
            success_rate = (modules_found / total_modules) * 100
            
            logger.info(f"ğŸ“Š Modules trouvÃ©s: {modules_found}/{total_modules} ({success_rate:.1f}%)")
            
            if modules_missing:
                logger.error(f"âŒ Modules manquants: {', '.join(modules_missing)}")
                self.results['critical_issues'].append(f"Missing modules: {', '.join(modules_missing)}")
            
            # ConsidÃ©rer comme rÃ©ussi si au moins 80% des modules sont prÃ©sents
            if success_rate >= 80:
                logger.info("âœ… Validation des modules rÃ©ussie")
                self.results['tests_passed'] += 1
                return True
            else:
                logger.error(f"âŒ Trop de modules manquants: {success_rate:.1f}%")
                self.results['tests_failed'] += 1
                self.results['critical_issues'].append(f"Too many missing modules: {success_rate:.1f}%")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Erreur dans la validation des modules: {e}")
            self.results['tests_failed'] += 1
            self.results['critical_issues'].append(f"Module validation error: {e}")
            return False
    
    def validate_specific_module(self, module_key: str, module_data: Any, module_name: str):
        """
        Validation spÃ©cifique pour chaque module
        """
        try:
            if module_key == 'visibility_results':
                # VÃ©rifier la structure des rÃ©sultats de visibilitÃ©
                if isinstance(module_data, dict):
                    required_fields = ['overall_visibility', 'platform_scores', 'queries_tested']
                    missing = [f for f in required_fields if f not in module_data]
                    if missing:
                        self.results['minor_issues'].append(f"Visibility results missing fields: {missing}")
                    else:
                        logger.info(f"    ğŸ“Š VisibilitÃ© globale: {module_data.get('overall_visibility', 0):.1%}")
                        logger.info(f"    ğŸ“Š RequÃªtes testÃ©es: {module_data.get('queries_tested', 0)}")
            
            elif module_key == 'semantic_analysis':
                # VÃ©rifier l'analyse sÃ©mantique
                if isinstance(module_data, dict):
                    industry = module_data.get('industry_classification', {})
                    if industry:
                        logger.info(f"    ğŸ­ Industrie dÃ©tectÃ©e: {industry.get('primary_industry', 'N/A')}")
                    
                    entities = module_data.get('entities', {})
                    if entities:
                        offerings = entities.get('offerings', [])
                        logger.info(f"    ğŸ¯ Offres identifiÃ©es: {len(offerings)}")
            
            elif module_key == 'competitive_intelligence':
                # VÃ©rifier l'intelligence compÃ©titive
                if isinstance(module_data, dict):
                    competitors_analyzed = module_data.get('competitors_analyzed', 0)
                    logger.info(f"    ğŸ† CompÃ©titeurs analysÃ©s: {competitors_analyzed}")
            
            elif module_key == 'schemas':
                # VÃ©rifier les schemas gÃ©nÃ©rÃ©s
                if isinstance(module_data, dict):
                    schema_types = [k for k in module_data.keys() if k != 'implementation_guide']
                    logger.info(f"    ğŸ“‹ Types de schemas: {len(schema_types)}")
            
            elif module_key == 'generated_articles':
                # VÃ©rifier les articles gÃ©nÃ©rÃ©s
                if isinstance(module_data, list):
                    logger.info(f"    ğŸ“ Articles gÃ©nÃ©rÃ©s: {len(module_data)}")
                    
        except Exception as e:
            logger.warning(f"    âš ï¸ Erreur validation spÃ©cifique {module_name}: {e}")
    
    def test_downloads(self) -> bool:
        """
        Test 7: VÃ©rifier les tÃ©lÃ©chargements (Dashboard HTML, Word DOCX, PDF)
        """
        logger.info("ğŸ” Test 7: Validation des tÃ©lÃ©chargements")
        self.results['tests_run'] += 1
        
        try:
            downloads = {
                'dashboard': {
                    'url': f"{API_BASE}/reports/{self.report_id}/dashboard",
                    'expected_content_type': 'text/html',
                    'name': 'Dashboard HTML'
                },
                'docx': {
                    'url': f"{API_BASE}/reports/{self.report_id}/docx",
                    'expected_content_type': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
                    'name': 'Rapport Word'
                },
                'pdf': {
                    'url': f"{API_BASE}/reports/{self.report_id}/pdf",
                    'expected_content_type': 'application/pdf',
                    'name': 'Rapport PDF'
                }
            }
            
            successful_downloads = 0
            failed_downloads = []
            
            for download_type, config in downloads.items():
                logger.info(f"ğŸ“¥ Test tÃ©lÃ©chargement {config['name']}")
                
                try:
                    start_time = time.time()
                    response = self.session.get(config['url'], timeout=30)
                    download_time = time.time() - start_time
                    
                    if response.status_code == 200:
                        content_type = response.headers.get('content-type', '').lower()
                        content_length = len(response.content)
                        
                        logger.info(f"  âœ… {config['name']}: {content_length} bytes en {download_time:.2f}s")
                        logger.info(f"     Content-Type: {content_type}")
                        
                        # VÃ©rifier le type de contenu
                        if config['expected_content_type'].lower() in content_type:
                            logger.info(f"     âœ… Type de contenu correct")
                        else:
                            logger.warning(f"     âš ï¸ Type de contenu inattendu (attendu: {config['expected_content_type']})")
                            self.results['minor_issues'].append(f"{config['name']} unexpected content type: {content_type}")
                        
                        # VÃ©rifier la taille minimale
                        min_size = 1000  # 1KB minimum
                        if content_length >= min_size:
                            logger.info(f"     âœ… Taille suffisante")
                            successful_downloads += 1
                        else:
                            logger.warning(f"     âš ï¸ Fichier trop petit: {content_length} bytes")
                            self.results['minor_issues'].append(f"{config['name']} file too small: {content_length} bytes")
                            successful_downloads += 1  # Pas critique
                        
                        self.results['downloads_validation'][download_type] = {
                            'success': True,
                            'size': content_length,
                            'content_type': content_type,
                            'download_time': download_time
                        }
                        
                    else:
                        logger.error(f"  âŒ {config['name']}: HTTP {response.status_code}")
                        failed_downloads.append(config['name'])
                        
                        self.results['downloads_validation'][download_type] = {
                            'success': False,
                            'status_code': response.status_code,
                            'error': response.text[:200]
                        }
                        
                except Exception as e:
                    logger.error(f"  âŒ {config['name']}: Erreur {e}")
                    failed_downloads.append(config['name'])
                    
                    self.results['downloads_validation'][download_type] = {
                        'success': False,
                        'error': str(e)
                    }
            
            # Ã‰valuation
            total_downloads = len(downloads)
            success_rate = (successful_downloads / total_downloads) * 100
            
            logger.info(f"ğŸ“Š TÃ©lÃ©chargements rÃ©ussis: {successful_downloads}/{total_downloads} ({success_rate:.1f}%)")
            
            if failed_downloads:
                logger.error(f"âŒ TÃ©lÃ©chargements Ã©chouÃ©s: {', '.join(failed_downloads)}")
                self.results['critical_issues'].append(f"Failed downloads: {', '.join(failed_downloads)}")
            
            if success_rate >= 66:  # Au moins 2/3 des tÃ©lÃ©chargements
                logger.info("âœ… Validation des tÃ©lÃ©chargements rÃ©ussie")
                self.results['tests_passed'] += 1
                return True
            else:
                logger.error(f"âŒ Trop de tÃ©lÃ©chargements Ã©chouÃ©s: {success_rate:.1f}%")
                self.results['tests_failed'] += 1
                self.results['critical_issues'].append(f"Too many failed downloads: {success_rate:.1f}%")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Erreur dans les tests de tÃ©lÃ©chargement: {e}")
            self.results['tests_failed'] += 1
            self.results['critical_issues'].append(f"Download test error: {e}")
            return False
    
    def check_backend_logs(self):
        """
        Test 8: VÃ©rifier les logs backend pour erreurs
        """
        logger.info("ğŸ” Test 8: VÃ©rification des logs backend")
        
        try:
            # VÃ©rifier les logs supervisor backend
            import subprocess
            
            log_files = [
                '/var/log/supervisor/backend.err.log',
                '/var/log/supervisor/backend.out.log'
            ]
            
            critical_errors = []
            warnings = []
            
            for log_file in log_files:
                try:
                    # Lire les derniÃ¨res lignes du log
                    result = subprocess.run(['tail', '-n', '50', log_file], 
                                          capture_output=True, text=True, timeout=10)
                    
                    if result.returncode == 0:
                        log_content = result.stdout
                        
                        # Chercher des erreurs critiques
                        error_keywords = ['ERROR', 'CRITICAL', 'Exception', 'Traceback', 'Failed']
                        warning_keywords = ['WARNING', 'WARN']
                        
                        lines = log_content.split('\n')
                        for line in lines[-20:]:  # DerniÃ¨res 20 lignes
                            if any(keyword in line for keyword in error_keywords):
                                critical_errors.append(line.strip())
                            elif any(keyword in line for keyword in warning_keywords):
                                warnings.append(line.strip())
                        
                        logger.info(f"ğŸ“‹ Log {log_file}: {len(lines)} lignes analysÃ©es")
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ Impossible de lire {log_file}: {e}")
            
            # Rapport des logs
            if critical_errors:
                logger.warning(f"âš ï¸ {len(critical_errors)} erreurs critiques trouvÃ©es dans les logs:")
                for error in critical_errors[-5:]:  # DerniÃ¨res 5 erreurs
                    logger.warning(f"  â€¢ {error}")
                self.results['minor_issues'].extend(critical_errors[-5:])
            
            if warnings:
                logger.info(f"ğŸ“‹ {len(warnings)} avertissements trouvÃ©s dans les logs")
            
            logger.info("âœ… VÃ©rification des logs terminÃ©e")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Erreur lors de la vÃ©rification des logs: {e}")
    
    def generate_final_report(self):
        """
        GÃ©nÃ¨re le rapport final des tests
        """
        self.results['test_end'] = datetime.now().isoformat()
        
        logger.info("\n" + "="*80)
        logger.info("ğŸ“‹ RAPPORT FINAL - TESTS COMPLETS BACKEND GEO SAAS")
        logger.info("="*80)
        
        logger.info(f"ğŸŒ Backend URL: {self.results['backend_url']}")
        logger.info(f"ğŸ¯ URL de test: {self.results['test_url']}")
        logger.info(f"ğŸ• DurÃ©e: {self.results['test_start']} â†’ {self.results['test_end']}")
        logger.info(f"ğŸ“Š Tests exÃ©cutÃ©s: {self.results['tests_run']}")
        logger.info(f"âœ… Tests rÃ©ussis: {self.results['tests_passed']}")
        logger.info(f"âŒ Tests Ã©chouÃ©s: {self.results['tests_failed']}")
        
        success_rate = (self.results['tests_passed'] / self.results['tests_run']) * 100 if self.results['tests_run'] > 0 else 0
        logger.info(f"ğŸ“ˆ Taux de rÃ©ussite: {success_rate:.1f}%")
        
        # MÃ©triques de performance
        if self.results['performance_metrics']:
            logger.info("\nâ±ï¸ MÃ‰TRIQUES DE PERFORMANCE:")
            for metric, value in self.results['performance_metrics'].items():
                logger.info(f"  â€¢ {metric}: {value:.2f}s")
        
        # Validation des modules
        if self.results['modules_validation']:
            logger.info("\nğŸ“‹ VALIDATION DES MODULES:")
            for module_key, module_info in self.results['modules_validation'].items():
                status = "âœ…" if module_info['present'] else "âŒ"
                logger.info(f"  {status} {module_info['name']}")
        
        # Validation des tÃ©lÃ©chargements
        if self.results['downloads_validation']:
            logger.info("\nğŸ“¥ VALIDATION DES TÃ‰LÃ‰CHARGEMENTS:")
            for download_type, download_info in self.results['downloads_validation'].items():
                status = "âœ…" if download_info['success'] else "âŒ"
                size = f" ({download_info.get('size', 0)} bytes)" if download_info['success'] else ""
                logger.info(f"  {status} {download_type.upper()}{size}")
        
        # ProblÃ¨mes critiques
        if self.results['critical_issues']:
            logger.info("\nğŸš¨ PROBLÃˆMES CRITIQUES:")
            for issue in self.results['critical_issues']:
                logger.info(f"  â€¢ {issue}")
        
        # ProblÃ¨mes mineurs
        if self.results['minor_issues']:
            logger.info(f"\nâš ï¸ PROBLÃˆMES MINEURS ({len(self.results['minor_issues'])}):")
            for issue in self.results['minor_issues'][:10]:  # Limiter Ã  10
                logger.info(f"  â€¢ {issue}")
            if len(self.results['minor_issues']) > 10:
                logger.info(f"  ... et {len(self.results['minor_issues']) - 10} autres")
        
        # IDs pour rÃ©fÃ©rence
        if self.lead_id:
            logger.info(f"\nğŸ”— RÃ‰FÃ‰RENCES:")
            logger.info(f"  â€¢ Lead ID: {self.lead_id}")
            logger.info(f"  â€¢ Job ID: {self.job_id}")
            logger.info(f"  â€¢ Report ID: {self.report_id}")
        
        # Sauvegarder le rapport complet
        with open('/app/backend_test_results_complete.json', 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"\nğŸ’¾ Rapport dÃ©taillÃ© sauvegardÃ©: /app/backend_test_results_complete.json")
        logger.info("="*80)


def main():
    """
    Point d'entrÃ©e principal
    """
    print("ğŸš€ TEST COMPLET BACKEND GEO SAAS")
    print("="*50)
    
    tester = GEOSaaSBackendTester()
    success = tester.run_complete_tests()
    
    if success:
        print("\nğŸ‰ TOUS LES TESTS SONT PASSÃ‰S!")
        return 0
    else:
        print("\nâŒ CERTAINS TESTS ONT Ã‰CHOUÃ‰!")
        return 1


if __name__ == "__main__":
    sys.exit(main())