"""
Service de cache local avec fichiers JSON
√âvite les appels API co√ªteux pour les analyses r√©centes
"""
import json
import hashlib
import logging
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, Any, Callable
from functools import wraps

from config import (
    CACHE_DIR,
    CACHE_TTL_HOURS,
    CACHE_ENABLED,
    is_cache_enabled
)

logger = logging.getLogger(__name__)


class CacheService:
    """Service de cache simple bas√© sur des fichiers JSON"""
    
    def __init__(self, cache_dir: Path = CACHE_DIR):
        self.cache_dir = cache_dir
        self.cache_dir.mkdir(exist_ok=True)
        self.enabled = CACHE_ENABLED
    
    def _get_cache_key_hash(self, key: str) -> str:
        """G√©n√®re un hash MD5 pour la cl√© de cache"""
        return hashlib.md5(key.encode()).hexdigest()
    
    def _get_cache_file_path(self, key: str) -> Path:
        """Retourne le chemin du fichier de cache"""
        key_hash = self._get_cache_key_hash(key)
        return self.cache_dir / f"{key_hash}.json"
    
    def get(self, key: str, max_age_hours: Optional[int] = None) -> Optional[Any]:
        """
        R√©cup√®re une valeur depuis le cache
        
        Args:
            key: Cl√© de cache
            max_age_hours: √Çge maximum en heures (d√©faut: CACHE_TTL_HOURS)
            
        Returns:
            Valeur cach√©e ou None si pas trouv√©e/expir√©e
        """
        if not self.enabled or not is_cache_enabled():
            return None
        
        max_age = max_age_hours if max_age_hours is not None else CACHE_TTL_HOURS
        cache_file = self._get_cache_file_path(key)
        
        if not cache_file.exists():
            return None
        
        try:
            # V√©rifier l'√¢ge du fichier
            file_time = datetime.fromtimestamp(cache_file.stat().st_mtime)
            age = datetime.now() - file_time
            
            if age > timedelta(hours=max_age):
                # Cache expir√©, supprimer
                cache_file.unlink()
                logger.debug(f"Cache expired for key: {key[:50]}...")
                return None
            
            # Lire le cache
            with open(cache_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                logger.info(f"‚úÖ Cache HIT for key: {key[:50]}...")
                return data.get('value')
                
        except Exception as e:
            logger.warning(f"Failed to read cache for {key[:50]}...: {e}")
            # En cas d'erreur, supprimer le fichier corrompu
            try:
                cache_file.unlink()
            except Exception:
                pass
            return None
    
    def set(self, key: str, value: Any) -> bool:
        """
        Sauvegarde une valeur dans le cache
        
        Args:
            key: Cl√© de cache
            value: Valeur √† cacher (doit √™tre JSON serializable)
            
        Returns:
            True si succ√®s, False sinon
        """
        if not self.enabled or not is_cache_enabled():
            return False
        
        cache_file = self._get_cache_file_path(key)
        
        try:
            data = {
                'key': key,
                'value': value,
                'cached_at': datetime.now().isoformat()
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"üíæ Cache SET for key: {key[:50]}...")
            return True
            
        except Exception as e:
            logger.error(f"Failed to write cache for {key[:50]}...: {e}")
            return False
    
    def delete(self, key: str) -> bool:
        """
        Supprime une entr√©e du cache
        
        Args:
            key: Cl√© √† supprimer
            
        Returns:
            True si supprim√©, False si pas trouv√©
        """
        cache_file = self._get_cache_file_path(key)
        
        if cache_file.exists():
            try:
                cache_file.unlink()
                logger.info(f"üóëÔ∏è  Cache DELETE for key: {key[:50]}...")
                return True
            except Exception as e:
                logger.error(f"Failed to delete cache for {key[:50]}...: {e}")
                return False
        
        return False
    
    def clear_all(self) -> int:
        """
        Vide tout le cache
        
        Returns:
            Nombre de fichiers supprim√©s
        """
        count = 0
        
        for cache_file in self.cache_dir.glob("*.json"):
            try:
                cache_file.unlink()
                count += 1
            except Exception as e:
                logger.error(f"Failed to delete {cache_file}: {e}")
        
        logger.info(f"üóëÔ∏è  Cleared {count} cache files")
        return count
    
    def cleanup_expired(self, max_age_hours: Optional[int] = None) -> int:
        """
        Supprime les fichiers de cache expir√©s
        
        Args:
            max_age_hours: √Çge maximum (d√©faut: CACHE_TTL_HOURS)
            
        Returns:
            Nombre de fichiers supprim√©s
        """
        max_age = max_age_hours if max_age_hours is not None else CACHE_TTL_HOURS
        cutoff_time = datetime.now() - timedelta(hours=max_age)
        count = 0
        
        for cache_file in self.cache_dir.glob("*.json"):
            try:
                file_time = datetime.fromtimestamp(cache_file.stat().st_mtime)
                if file_time < cutoff_time:
                    cache_file.unlink()
                    count += 1
            except Exception as e:
                logger.error(f"Failed to cleanup {cache_file}: {e}")
        
        if count > 0:
            logger.info(f"üóëÔ∏è  Cleaned up {count} expired cache files")
        
        return count
    
    def get_cache_stats(self) -> dict:
        """
        Retourne des statistiques sur le cache
        
        Returns:
            Dictionnaire avec les stats
        """
        cache_files = list(self.cache_dir.glob("*.json"))
        total_size = sum(f.stat().st_size for f in cache_files)
        
        return {
            'enabled': self.enabled,
            'total_files': len(cache_files),
            'total_size_mb': round(total_size / (1024 * 1024), 2),
            'cache_dir': str(self.cache_dir)
        }


def cache_result(key_prefix: str, max_age_hours: Optional[int] = None):
    """
    D√©corateur pour cacher automatiquement le r√©sultat d'une fonction
    
    Usage:
        @cache_result("analysis", max_age_hours=168)
        async def analyze_site(url: str):
            ...
    """
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            cache = CacheService()
            
            # G√©n√©rer une cl√© unique bas√©e sur les arguments
            cache_key = f"{key_prefix}:{str(args)}:{str(kwargs)}"
            
            # V√©rifier le cache
            cached_value = cache.get(cache_key, max_age_hours)
            if cached_value is not None:
                return cached_value
            
            # Ex√©cuter la fonction
            result = await func(*args, **kwargs)
            
            # Cacher le r√©sultat
            cache.set(cache_key, result)
            
            return result
        
        return wrapper
    return decorator


# Instance globale du cache
cache_service = CacheService()
